from __future__ import print_function
"""

"""
import pickle

from pyspark.ml.feature import HashingTF, IDF, Tokenizer, ChiSqSelectorModel, Binarizer
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

import pyspark.sql.functions as F

naive_bayes_path = "output/naive_bayes"
naive_bayes_model_path = "output/naive_bayes_model"

chi_model_path = "../explore/output/chi_model"
feature_path = "../explore/output/features.pkl"

feature_cols = None
model = None

try:
	model = ChiSqSelectorModel.load(chi_model_path)

	with open(feature_path, "rb") as f:
		feature_cols = pickle.load(f)
except:
	print("WARN: Output path missing")

selected_cols = [feature_cols[i] for i in model.selectedFeatures]
print("Selected Features Count: {0}".format(len(selected_cols)))
print("Selected Features: {0}".format(selected_cols))

df = spark.read.load("../datasets/train.csv", format="csv", sep=",", inferSchema="true", header="true")

label_col = ["HasDetections"]
meta_cols = ["MachineIdentifier"]
exclude_cols = ["DefaultBrowsersIdentifier", "OrganizationIdentifier", "GeoNameIdentifier", "LocaleEnglishNameIdentifier", "Census_ProcessorCoreCount", "Census_ProcessorClass", "Census_PrimaryDiskTotalCapacity", "Census_PrimaryDiskTypeName", "Census_SystemVolumeTotalCapacity", "Census_HasOpticalDiskDrive", "Census_TotalPhysicalRAM", "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Census_InternalPrimaryDisplayResolutionHorizontal", "Census_InternalBatteryType", "Census_InternalBatteryNumberOfCharges"]
model_cols = list(set(selected_cols) - set(exclude_cols))

print("Selected Features without Exclusions Count: {0}".format(len(model_cols)))

for column_name in model_cols:
	df = df.withColumn(column_name + "_Item", F.concat(F.lit(column_name + "_"), F.col(column_name)))
	
item_cols = [column_name for column_name in df.columns if column_name.endswith("_Item")]
ordered_cols = list(label_col + meta_cols + item_cols)
df = df.select(*ordered_cols)

df = df.withColumn("items", F.concat_ws(" ", *item_cols))

print("Performing Tokenization")
tokenizer = Tokenizer(inputCol="items", outputCol="tokens")
df_tokens = tokenizer.transform(df)

print("Performing Token Hashing")
hasher = HashingTF(inputCol="tokens", outputCol="hashed_features", numFeatures=512)
df_hashed = hasher.transform(df_tokens)

'''
print("Splitting into Training and Test Datasets")
splits = df_hashed.randomSplit([0.6, 0.4], 51)
train = splits[0]
test = splits[1]

print("Fitting - Training Data")
naive_bayes = NaiveBayes(smoothing=1.0, featuresCol="hashed_features", labelCol="HasDetections", predictionCol="prediction", modelType="multinomial")
naive_model = naive_bayes.fit(train)
'''

print("Performing IDF")
idf = IDF(inputCol="hashed_features", outputCol="idf_features")
idf_model = idf.fit(df_hashed)
df_idf = idf_model.transform(df_hashed)

print("Splitting into Training and Test Datasets")
splits = df_idf.randomSplit([0.6, 0.4], 51)
train = splits[0]
test = splits[1]

print("Fitting - Training Data")
naive_bayes = NaiveBayes(smoothing=1.0, featuresCol="idf_features", labelCol="HasDetections", predictionCol="prediction", modelType="multinomial")
naive_model = naive_bayes.fit(train)

print("Saving Models")
naive_bayes.write().overwrite().save(naive_bayes_path)
naive_model.write().overwrite().save(naive_bayes_model_path)

print("Fitting - Test Data")
predictions = naive_model.transform(test)
predictions.select("HasDetections", "MachineIdentifier", "probability", "prediction").show(truncate=False)

#print("Saving Predictions")
#predictions.write.saveAsTable("naive_bayes_predictions_initial", format="parquet", mode="overwrite", path="output/tables/naive_bayes/predictions_initial")

print("Computing Multiclass Accuracy")
evaluator = MulticlassClassificationEvaluator(labelCol="HasDetections", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Test set accuracy = " + str(accuracy))



