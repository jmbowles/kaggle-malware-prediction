from __future__ import print_function
"""
Bayes Rule:

P(Malware | X) = P(X | Malware) P(Malware) / P(X)

Where X = 83 column values in each dataset:
"""
import pickle

from pyspark.sql import Window
from pyspark.ml import Pipeline
from pyspark.ml.feature import FeatureHasher, PCA
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

import pyspark.sql.functions as F

pipeline_model_path = "output/logistic_pipeline_model"

#selected_cols = ["Platform", "AvSigVersion", "AVProductStatesIdentifier", "CountryIdentifier", "SMode", "Wdft_RegionIdentifier", "Census_OSVersion", "OsBuildLab", "Census_IsTouchEnabled", "Census_IsPenCapable", "Census_IsSecureBootEnabled"]
#selected_cols = ["Platform", "AvSigVersion", "AVProductStatesIdentifier", "CountryIdentifier", "Census_OSVersion", "Derived_Firmware", "Derived_Processor", "Derived_Installer", "Derived_AV", "Derived_Grouping", "Census_IsTouchEnabled", "Census_IsSecureBootEnabled"]
print("Loading and Caching Data")
df = spark.read.load("../datasets/train.csv", format="csv", sep=",", inferSchema="true", header="true")
df.cache()

#df = df.withColumn("Derived_Firmware", F.concat(df.Census_FirmwareManufacturerIdentifier, F.lit("_"), df.Census_FirmwareVersionIdentifier))
df = df.replace("requireAdmin", "RequireAdmin", ["SmartScreen"])
df = df.replace("on", "1", ["SmartScreen"])
df = df.replace("On", "1", ["SmartScreen"])
df = df.replace("Enabled", "1", ["SmartScreen"])
df = df.replace("prompt", "Prompt", ["SmartScreen"])
df = df.replace("Promt", "Prompt", ["SmartScreen"])
df = df.replace("00000000", "0", ["SmartScreen"])
df = df.replace("off", "0", ["SmartScreen"])
df = df.replace("OFF", "0", ["SmartScreen"])
df = df.replace("warn", "Warn", ["SmartScreen"])
df = df.fillna("0", ["SmartScreen"])

exclude_cols = ["HasDetections", "MachineIdentifier", "CityIdentifier", "AutoSampleOptIn", "LocaleEnglishNameIdentifier", "DefaultBrowsersIdentifier", "PuaMode", "Census_IsFlightingInternal", "OrganizationIdentifier", "GeoNameIdentifier", "LocaleEnglishNameIdentifier", "Census_ProcessorCoreCount", "Census_ProcessorClass", "Census_PrimaryDiskTotalCapacity", "Census_PrimaryDiskTypeName", "Census_SystemVolumeTotalCapacity", "Census_HasOpticalDiskDrive", "Census_TotalPhysicalRAM", "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Census_InternalPrimaryDisplayResolutionHorizontal", "Census_InternalBatteryType", "Census_InternalBatteryNumberOfCharges"]
selected_cols = list(set(df.columns) - set(exclude_cols))
#selected_cols = ["SmartScreen", "Derived_Firmware", "Census_ProcessorModelIdentifier", "AVProductStatesIdentifier", "AvSigVersion", "Census_OSVersion"]

print("Selected Features Count: {0}".format(len(selected_cols)))
print("Selected Features: {0}".format(selected_cols))

print("Creating Splits")
train, test = df.randomSplit([0.7, 0.3])

print("Building Pipeline")
hasher = FeatureHasher(numFeatures=32768, inputCols=selected_cols, outputCol="features")
evaluator = MulticlassClassificationEvaluator(labelCol="HasDetections", predictionCol="prediction", metricName="accuracy")
stages = []
stages.append(hasher)
stages.append(LogisticRegression(featuresCol="features", labelCol="HasDetections", standardization=False, maxIter=100, regParam=0.3, elasticNetParam=0.8, tol=1e-06, 
								 fitIntercept=True, threshold=0.5, family="auto"))
pipeline = Pipeline(stages=stages)

print("Configuring CrossValidation")
params = ParamGridBuilder() \
			.addGrid(hasher.numFeatures, [1024, 2048]) \
			.build()

validator = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=params,
                          evaluator=evaluator,
                          numFolds=3)

print("Fitting -> Training Data")
pipeline_model = validator.fit(train)

print("Fitting -> Test Data")
predictions = pipeline_model.transform(test)
predictions.select("HasDetections", "MachineIdentifier", "probability", "prediction").show(truncate=False)

print("FeatureHasher.numFeatures = {0}".format(stages[0].getNumFeatures()))
stages[0].getNumFeatures()

print("Computing Accuracy")
accuracy = evaluator.evaluate(predictions)
print("Test set accuracy = {0}".format(accuracy))

print("Saving Pipeline Model")
pipeline_model.bestModel.write().overwrite().save(pipeline_model_path)

#print("Saving Predictions")
#predictions.write.saveAsTable("naive_bayes_predictions", format="parquet", mode="overwrite", path="output/tables/naive_bayes/predictions")






