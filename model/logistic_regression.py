from __future__ import print_function
"""

accuracy                                                                    
0.5626835113558811


"""
import pickle

from pyspark.ml import Pipeline
from pyspark.ml.feature import FeatureHasher, MinMaxScaler, VectorAssembler, StringIndexer
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

import pyspark.sql.functions as F

pipeline_model_path = "output/logistic_pipeline_model"

print("Loading and Caching Data")
df = spark.read.load("../datasets/train.csv", format="csv", sep=",", inferSchema="true", header="true")
df.cache()

#df = df.withColumn("Derived_Firmware", F.concat(df.Census_FirmwareManufacturerIdentifier, F.lit("_"), df.Census_FirmwareVersionIdentifier))
df = df.replace("requireAdmin", "RequireAdmin", ["SmartScreen"])
df = df.replace("on", "1", ["SmartScreen"])
df = df.replace("On", "1", ["SmartScreen"])
df = df.replace("Enabled", "1", ["SmartScreen"])
df = df.replace("prompt", "Prompt", ["SmartScreen"])
df = df.replace("Promt", "Prompt", ["SmartScreen"])
df = df.replace("00000000", "0", ["SmartScreen"])
df = df.replace("off", "0", ["SmartScreen"])
df = df.replace("OFF", "0", ["SmartScreen"])
df = df.replace("warn", "Warn", ["SmartScreen"])
df = df.fillna("0", ["SmartScreen"])
df = df.withColumn("Derived_AvSigVersion", F.regexp_replace(df.AvSigVersion, r"[^0-9]", "").cast("integer"))
df = df.withColumn("Derived_AppVersion", F.regexp_replace(df.AppVersion, r"[^0-9]", "").cast("integer"))
df = df.withColumn("Derived_EngineVersion", F.regexp_replace(df.EngineVersion, r"[^0-9]", "").cast("integer"))
df = df.withColumn("Derived_OsVer", F.regexp_replace(df.OsVer, r"[^0-9]", "").cast("integer"))
df = df.withColumn("Derived_CensusOSVersion", F.regexp_replace(df.Census_OSVersion, r"[^0-9]", "").cast("integer"))

exclude_cols = ["HasDetections", "MachineIdentifier", "AvSigVersion", "AppVersion", "EngineVersion", "OsVer", "Census_OSVersion", "Census_OSBuildNumber", "Census_OSBuildRevision", "CityIdentifier", "AutoSampleOptIn", "LocaleEnglishNameIdentifier", "DefaultBrowsersIdentifier", "PuaMode", "Census_IsFlightingInternal", "OrganizationIdentifier", "GeoNameIdentifier", "LocaleEnglishNameIdentifier", "Census_ProcessorCoreCount", "Census_ProcessorClass", "Census_PrimaryDiskTotalCapacity", "Census_PrimaryDiskTypeName", "Census_SystemVolumeTotalCapacity", "Census_TotalPhysicalRAM", "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Census_InternalPrimaryDisplayResolutionHorizontal", "Census_InternalBatteryType", "Census_InternalBatteryNumberOfCharges"]
base_cols = list(set(df.columns) - set(exclude_cols))
derived_scaled = ["Derived_AvSigVersion_scaled", "Derived_AppVersion_scaled", "Derived_EngineVersion_scaled", "Derived_OsVer_scaled", "Derived_CensusOSVersion_scaled"]
derived_cols = ["Derived_AvSigVersion", "Derived_AppVersion", "Derived_EngineVersion", "Derived_OsVer", "Derived_CensusOSVersion"]
categorical_cols = [col for col, dtype in df.dtypes if dtype=="string" or "Is" in col]
categorical_cols = list(set(categorical_cols) - set(exclude_cols)) 

print("Creating Splits")
train, test = df.randomSplit([0.7, 0.3])

stages = []
category_cols = []

print("Indexing Categorical Features")
for category in categorical_cols:
	indexed = category + "_" + "indexed"
	category_cols.append(indexed)
	indexer = StringIndexer(inputCol=category, outputCol=indexed, handleInvalid="skip", stringOrderType="frequencyDesc")
	stages.append(indexer)

selected_cols = derived_scaled + category_cols

print("Selected Features Count: {0}".format(len(selected_cols)))
print("Selected Features: {0}".format(selected_cols))

print("Building Pipeline")
stages.append(VectorAssembler(inputCols=["Derived_AvSigVersion"], outputCol="Derived_AvSigVersion_vector", handleInvalid="skip"))
stages.append(VectorAssembler(inputCols=["Derived_AppVersion"], outputCol="Derived_AppVersion_vector", handleInvalid="skip"))
stages.append(VectorAssembler(inputCols=["Derived_EngineVersion"], outputCol="Derived_EngineVersion_vector", handleInvalid="skip"))
stages.append(VectorAssembler(inputCols=["Derived_OsVer"], outputCol="Derived_OsVer_vector", handleInvalid="skip"))
stages.append(VectorAssembler(inputCols=["Derived_CensusOSVersion"], outputCol="Derived_CensusOSVersion_vector", handleInvalid="skip"))
stages.append(MinMaxScaler(min=0.0, max=1.0, inputCol="Derived_AvSigVersion_vector", outputCol="Derived_AvSigVersion_scaled"))
stages.append(MinMaxScaler(min=0.0, max=1.0, inputCol="Derived_AppVersion_vector", outputCol="Derived_AppVersion_scaled"))
stages.append(MinMaxScaler(min=0.0, max=1.0, inputCol="Derived_EngineVersion_vector", outputCol="Derived_EngineVersion_scaled"))
stages.append(MinMaxScaler(min=0.0, max=1.0, inputCol="Derived_OsVer_vector", outputCol="Derived_OsVer_scaled"))
stages.append(MinMaxScaler(min=0.0, max=1.0, inputCol="Derived_CensusOSVersion_vector", outputCol="Derived_CensusOSVersion_scaled"))
stages.append(VectorAssembler(inputCols=selected_cols, outputCol="features", handleInvalid="skip"))
#hasher = FeatureHasher(numFeatures=32768, inputCols=selected_cols, outputCol="features")
evaluator = MulticlassClassificationEvaluator(labelCol="HasDetections", predictionCol="prediction", metricName="accuracy")
#stages.append(hasher)
regression = LogisticRegression(featuresCol="features", labelCol="HasDetections", standardization=False, maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-06, 
								 fitIntercept=True, threshold=0.5, family="auto")
stages.append(regression)
pipeline = Pipeline(stages=stages)


print("Configuring CrossValidation")
params = ParamGridBuilder() \
			.addGrid(regression.fitIntercept, [True, False]) \
			.addGrid(regression.maxIter, [100, 200]) \
			.addGrid(regression.standardization, [True, False]) \
			.build()

validator = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=params,
                          evaluator=evaluator,
                          numFolds=5)

print("Fitting -> Training Data")
pipeline_model = validator.fit(train)


#pipeline_model = pipeline.fit(train)

print("Fitting -> Test Data")
predictions = pipeline_model.transform(test)
predictions.select("HasDetections", "MachineIdentifier", "probability", "prediction").show(truncate=False)

print("Computing Accuracy")
accuracy = evaluator.evaluate(predictions)
print("Test set accuracy = {0}".format(accuracy))

print("Saving Pipeline Model")
pipeline_model.write().overwrite().save(pipeline_model_path)

#print("Saving Predictions")
#predictions.write.saveAsTable("naive_bayes_predictions", format="parquet", mode="overwrite", path="output/tables/naive_bayes/predictions")






