from __future__ import print_function
"""

2018-12-21 15:50	Submittal 1, NB, 15 features :: Leaderboard=434/460,  Score=0.511
2018-12-22 10:47	Submittal 2, NB, Tokenizer+HashingTF+IDF, 51 features, HashingTF=512 :: Leaderboard=423/471,  Score=0.558
2018-12-27 06:45	Submittal 3, NB Pipleline StringIndexer+OneHotEncoderEstimator+VectorAssembler, 8 features :: Score=0.595

"""
import pickle


from pyspark.sql.types import DoubleType
from pyspark.ml.feature import ChiSqSelectorModel, StringIndexer, VectorAssembler, OneHotEncoderEstimator
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml import PipelineModel

import pyspark.sql.functions as F

pipeline_path = "output/naive_bayes_pipeline"
pipeline_model_path = "output/naive_bayes_pipeline_model"
csv_path = "output/submit/submittal_3.csv"

chi_model_path = "../explore/output/chi_model"
feature_path = "../explore/output/features.pkl"

feature_cols = None
model = None

try:
	model = ChiSqSelectorModel.load(chi_model_path)

	with open(feature_path, "rb") as f:
		feature_cols = pickle.load(f)
except:
	print("WARN: Output path missing")

#selected_cols = [feature_cols[i] for i in model.selectedFeatures]
selected_cols = ["AVProductStatesIdentifier", "CountryIdentifier", "Wdft_RegionIdentifier", "OsBuildLab", "SkuEdition", "Census_PowerPlatformRoleName", "Census_OSVersion", "Census_OSInstallLanguageIdentifier"]

print("Selected Features Count: {0}".format(len(selected_cols)))
print("Selected Features: {0}".format(selected_cols))

test = spark.read.load("../datasets/test.csv", format="csv", sep=",", inferSchema="true", header="true")

meta_cols = ["MachineIdentifier"]
#exclude_cols = ["DefaultBrowsersIdentifier", "OrganizationIdentifier", "GeoNameIdentifier", "LocaleEnglishNameIdentifier", "Census_ProcessorCoreCount", "Census_ProcessorClass", "Census_PrimaryDiskTotalCapacity", "Census_PrimaryDiskTypeName", "Census_SystemVolumeTotalCapacity", "Census_HasOpticalDiskDrive", "Census_TotalPhysicalRAM", "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Census_InternalPrimaryDisplayResolutionHorizontal", "Census_InternalBatteryType", "Census_InternalBatteryNumberOfCharges"]
exclude_cols = []
#model_cols = list(set(selected_cols) - set(exclude_cols))

#print("Selected Features without Exclusions Count: {0}".format(len(model_cols)))

print("Loading Pipeline")
pipeline_model = PipelineModel.load(pipeline_model_path)

print("Fitting for Submittal")
predictions = pipeline_model.transform(test)
predictions.select("MachineIdentifier", "probability", "prediction").show(truncate=False)

print("Creating CSV for Submittal")

# Silly workaround for extracting an element from a dense or sparse vector. Probability column is a vector, with probs for each label
# https://stackoverflow.com/questions/39555864/how-to-access-element-of-a-vectorudt-column-in-a-spark-dataframe
def vector_item_(vector_column, index):
    try:
        return float(vector_column[index])
    except ValueError:
        return None

vector_item = F.udf(vector_item_, DoubleType())

df_submit = predictions.withColumn("Label_0", vector_item("probability", F.lit(0)))
df_submit = df_submit.withColumn("Label_1", vector_item("probability", F.lit(1)))
df_submit = df_submit.withColumn("HasDetections", df_submit.Label_1)
df_submit = df_submit.select("MachineIdentifier", "HasDetections")

# Yet another workaround to write to a CSV file
df_submit.coalesce(1).toPandas().to_csv(csv_path, header=True, index=False)

print("Total rows written to file: {0}".format(df_submit.count()))







