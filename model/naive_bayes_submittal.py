from __future__ import print_function
"""

Initial run:

Test set accuracy: 0.826381163685
Total Predictions on Test Dataset: 3,568,766

"""
import pickle


from pyspark.sql.types import DoubleType
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, ChiSqSelectorModel
from pyspark.ml.feature import ChiSqSelectorModel
from pyspark.ml.classification import NaiveBayes, NaiveBayesModel
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

import pyspark.sql.functions as F

naive_bayes_path = "output/naive_bayes"
naive_bayes_model_path = "output/naive_bayes_model"
csv_path = "output/csv/submittal_initial"

chi_model_path = "../explore/output/chi_model"
feature_path = "../explore/output/features.pkl"

feature_cols = None
model = None

try:
	model = ChiSqSelectorModel.load(chi_model_path)

	with open(feature_path, "rb") as f:
		feature_cols = pickle.load(f)
except:
	print("WARN: Output path missing")

selected_cols = [feature_cols[i] for i in model.selectedFeatures]
print("Selected Features Count: {0}".format(len(selected_cols)))
print("Selected Features: {0}".format(selected_cols))

df = spark.read.load("../datasets/test.csv", format="csv", sep=",", inferSchema="true", header="true")

meta_cols = ["MachineIdentifier"]
exclude_cols = ["DefaultBrowsersIdentifier", "OrganizationIdentifier", "GeoNameIdentifier", "LocaleEnglishNameIdentifier", "Census_ProcessorCoreCount", "Census_ProcessorClass", "Census_PrimaryDiskTotalCapacity", "Census_PrimaryDiskTypeName", "Census_SystemVolumeTotalCapacity", "Census_HasOpticalDiskDrive", "Census_TotalPhysicalRAM", "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Census_InternalPrimaryDisplayResolutionHorizontal", "Census_InternalBatteryType", "Census_InternalBatteryNumberOfCharges"]
model_cols = list(set(selected_cols) - set(exclude_cols))

print("Selected Features without Exclusions Count: {0}".format(len(model_cols)))

for column_name in model_cols:
	df = df.withColumn(column_name + "_Item", F.concat(F.lit(column_name + "_"), F.col(column_name)))
	
item_cols = [column_name for column_name in df.columns if column_name.endswith("_Item")]
ordered_cols = list(meta_cols + item_cols)
df = df.select(*ordered_cols)

df = df.withColumn("items", F.concat_ws(" ", *item_cols))

print("Performing Tokenization")
tokenizer = Tokenizer(inputCol="items", outputCol="tokens")
df_tokens = tokenizer.transform(df)

print("Performing Token Hashing")
hasher = HashingTF(inputCol="tokens", outputCol="hashed_features", numFeatures=len(item_cols))
df_hashed = hasher.transform(df_tokens)

print("Performing IDF")
idf = IDF(inputCol="hashed_features", outputCol="idf_features")
idf_model = idf.fit(df_hashed)
test = idf_model.transform(df_hashed)

print("Loading NaiveBayes Model")
naive_model = NaiveBayesModel.load(naive_bayes_model_path)

print("Fitting for Submittal")
predictions = naive_model.transform(test)
predictions.select("MachineIdentifier", "probability", "prediction").show(truncate=False)

print("Saving Submitted Predictions")
predictions.write.saveAsTable("naive_bayes_submitted_predictions_initial", format="parquet", mode="overwrite", path="output/tables/naive_bayes/submitted_predictions_initial")

print("Creating CSV for Submittal")

# Silly workaround for extracting an element from a dense or sparse vector. Probability column is a vector, with probs for each label
# https://stackoverflow.com/questions/39555864/how-to-access-element-of-a-vectorudt-column-in-a-spark-dataframe
def vector_item_(vector_column, index):
    try:
        return float(vector_column[index])
    except ValueError:
        return None

vector_item = F.udf(vector_item_, DoubleType())

df_submit = predictions.withColumn("Label_0", vector_item("probability", F.lit(0)))
df_submit = df_submit.withColumn("Label_1", vector_item("probability", F.lit(1)))
df_submit = df_submit.withColumn("HasDetections", df_submit.Label_1)
df_submit = df_submit.select("MachineIdentifier", "HasDetections")

df_submit.coalesce(1).write.csv(path=csv_path, mode="overwrite", header="true")





