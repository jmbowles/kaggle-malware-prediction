from __future__ import print_function
"""


"""
from pyspark.sql.types import DoubleType
from pyspark.ml import PipelineModel

import pyspark.sql.functions as F
from pyspark.sql import SparkSession

csv_path = "output/submit/submittal_8.csv"

spark = SparkSession.builder.appName("EnsembleSubmittal") \
	.config("spark.dynamicAllocation.enabled", "true") \
	.config("spark.shuffle.service.enabled", "true") \
	.config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
	.enableHiveSupport() \
	.getOrCreate()


def transform(df, model_path, prediction_column):
	pipeline_model = PipelineModel.load(model_path)
	predictions = pipeline_model.transform(df)
	predictions = predictions.withColumnRenamed("prediction", prediction_column)
	return predictions


print("Loading and Caching Data")
df = spark.read.table("test")
df.cache()

print("Transforming Model Predictions")
predictions = transform(df, "output/logistic_pipeline_model", "logistic_prediction")
predictions = transform(predictions, "output/naive_bayes_pipeline_model", "bayes_prediction")
predictions = transform(predictions, "output/gradient_boosted_pipeline_model", "boosted_prediction")
predictions = transform(predictions, "output/perceptron_pipeline_model", "perceptron_prediction")
predictions = transform(predictions, "output/forest_pipeline_model", "forest_prediction")
predictions = transform(predictions, "output/one_rest_pipeline_model", "onerest_prediction")

predict_cols = [col for col in df.columns if col.endswith("_prediction")]

predictions.select("MachineIdentifier", "probability", "prediction").show(truncate=False)

print("Creating CSV for Submittal")

# Silly workaround for extracting an element from a dense or sparse vector. Probability column is a vector, with probs for each label
# https://stackoverflow.com/questions/39555864/how-to-access-element-of-a-vectorudt-column-in-a-spark-dataframe
def vector_item_(vector_column, index):
    try:
        return float(vector_column[index])
    except ValueError:
        return None

vector_item = F.udf(vector_item_, DoubleType())

df_submit = predictions.withColumn("Label_0", vector_item("probability", F.lit(0)))
df_submit = df_submit.withColumn("Label_1", vector_item("probability", F.lit(1)))
df_submit = df_submit.withColumn("HasDetections", df_submit.Label_1)
df_submit = df_submit.select("MachineIdentifier", "HasDetections")

# Yet another workaround to write to a CSV file
df_submit.coalesce(1).toPandas().to_csv(csv_path, header=True, index=False)

print("Total rows written to file: {0}".format(df_submit.count()))