from __future__ import print_function
"""

Determining Hidden Layers:

https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw
"""
import pickle

from pyspark.ml import Pipeline
from pyspark.ml.feature import FeatureHasher
from pyspark.ml.classification import MultilayerPerceptronClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.sql import SparkSession

import pyspark.sql.functions as F

pipeline_model_path = "output/perceptron_pipeline_model"

print("Loading and Caching Data")

spark = SparkSession.builder.appName("MultilayerPerceptronClassifier") \
	.config("spark.dynamicAllocation.enabled", "true") \
	.config("spark.shuffle.service.enabled", "true") \
	.getOrCreate()

df = spark.read.load("../datasets/train.csv", format="csv", sep=",", inferSchema="true", header="true")
df.cache()

#df = df.withColumn("Derived_Firmware", F.concat(df.Census_FirmwareManufacturerIdentifier, F.lit("_"), df.Census_FirmwareVersionIdentifier))
df = df.replace("requireAdmin", "RequireAdmin", ["SmartScreen"])
df = df.replace("on", "1", ["SmartScreen"])
df = df.replace("On", "1", ["SmartScreen"])
df = df.replace("Enabled", "1", ["SmartScreen"])
df = df.replace("prompt", "Prompt", ["SmartScreen"])
df = df.replace("Promt", "Prompt", ["SmartScreen"])
df = df.replace("00000000", "0", ["SmartScreen"])
df = df.replace("off", "0", ["SmartScreen"])
df = df.replace("OFF", "0", ["SmartScreen"])
df = df.replace("warn", "Warn", ["SmartScreen"])
df = df.fillna("0", ["SmartScreen"])


exclude_cols = ["HasDetections", "MachineIdentifier", "CityIdentifier", "AutoSampleOptIn", "LocaleEnglishNameIdentifier", "DefaultBrowsersIdentifier", "PuaMode", "Census_IsFlightingInternal", "OrganizationIdentifier", "GeoNameIdentifier", "LocaleEnglishNameIdentifier", "Census_ProcessorCoreCount", "Census_ProcessorClass", "Census_PrimaryDiskTotalCapacity", "Census_PrimaryDiskTypeName", "Census_SystemVolumeTotalCapacity", "Census_TotalPhysicalRAM", "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Census_InternalPrimaryDisplayResolutionHorizontal", "Census_InternalBatteryType", "Census_InternalBatteryNumberOfCharges"]
selected_cols = list(set(df.columns) - set(exclude_cols)) 

print("Creating Splits")
train, test = df.randomSplit([0.7, 0.3])

print("Selected Features Count: {0}".format(len(selected_cols)))
print("Selected Features: {0}".format(selected_cols))

print("Building Pipeline")
hasher = FeatureHasher(numFeatures=512, inputCols=selected_cols, outputCol="features")
neural = MultilayerPerceptronClassifier(featuresCol="features", labelCol='HasDetections', predictionCol="prediction", seed=64, layers=[512, 32, 2], probabilityCol="probability")

pipeline = Pipeline(stages=[hasher, neural])
evaluator = MulticlassClassificationEvaluator(labelCol="HasDetections", predictionCol="prediction", metricName="accuracy")

print("Configuring CrossValidation")
params = ParamGridBuilder() \
			.addGrid(hasher.numFeatures, [512]) \
			.addGrid(neural.blockSize, [128]) \
			.addGrid(neural.maxIter, [200]) \
			.addGrid(neural.stepSize, [0.03]) \
			.addGrid(neural.solver, ["l-bfgs"]) \
			.build()

validator = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=params,
                          evaluator=evaluator,
                          numFolds=2)

print("Fitting -> Training Data")
pipeline_model = validator.fit(train)

print("Fitting -> Test Data")
predictions = pipeline_model.transform(test)
predictions.select("HasDetections", "MachineIdentifier", "probability", "prediction").show(truncate=False)

print("Computing Accuracy")
accuracy = evaluator.evaluate(predictions)
print("Test set accuracy = {0}".format(accuracy))

print("Saving Pipeline Model")
pipeline_model.bestModel.write().overwrite().save(pipeline_model_path)

#print("Saving Predictions")
#predictions.write.saveAsTable("perceptron_predictions", format="parquet", mode="overwrite", path="output/tables/perceptron/predictions")






